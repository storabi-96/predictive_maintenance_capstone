---
title: "Predictive Maintenance: Capstone Project Report"
subtitle: "HarvardX PH125.9x - Data Science: Capstone"
author: "Seyed M. Seyedtorabi"
date: "2024-Feb-14"
output: pdf_document
---

\newpage

```{r libraries and functions, include=FALSE}
# installing the libraries
if(!require(tidyverse   )) install.packages("tidyverse"  , repos = "http://cran.us.r-project.org")
if(!require(caret       )) install.packages("caret"      , repos = "http://cran.us.r-project.org")
if(!require(corrplot    )) install.packages("corrplot"   , repos = "http://cran.us.r-project.org")
if(!require(GGally      )) install.packages("GGally"     , repos = "http://cran.us.r-project.org")
if(!require(ROCR        )) install.packages("ROCR"       , repos = "http://cran.us.r-project.org")
if(!require(klaR        )) install.packages("klaR"       , repos = "http://cran.us.r-project.org")
if(!require(xgboost     )) install.packages("xgboost"    , repos = "http://cran.us.r-project.org")
if(!require(pROC        )) install.packages("pROC"       , repos = "http://cran.us.r-project.org")
if(!require(smotefamily )) install.packages("smotefamily", repos = "http://cran.us.r-project.org")
if(!require(knitr       )) install.packages("knitr"      , repos = "http://cran.us.r-project.org")

library(knitr) 
library(corrplot)
library(tidyverse)
library(caret)
library(GGally)
library(ROCR)
library(pROC)
library(smotefamily)
library(UBL)
library(MASS)
################################################################################
############### Defining functions to be used in script#########################
################################################################################

# function for one_hot encoding of categorical feature (type in our case)
one_hot_encode <- function(df_data, categorical_col){
  # Ensure the specified column is categorical
  if (!is.factor(df_data[[categorical_col]])) {
    stop("Specified column is not categorical.")
  }
  
  # Get unique categories in the categorical column
  categories <- levels(df_data[[categorical_col]])
  
  # Create binary columns for each category
  for (category in categories) {
    binary_col <- ifelse(df_data[[categorical_col]] == category, 1, 0)
    col_name <- paste(categorical_col, category, sep = "_")
    df_data[[col_name]] <- binary_col
  }
  
  # Return the encoded dataframe
  df_data
}

################################################################################
train_model <- function(df_training, target,
                        features, method, control){
  # declare the formula (y ~ x1 + x2 + etc.)
  formula <- as.formula(
    paste(
      target, "~", paste(features, collapse = " + ")))
  
  # train the model
  model <- train(formula,
                 data = df_training,
                 method = method,
                 trControl = control)
  model
}

################################################################################
get_confusion_matrix <- function(model, df_test, target) {
  # make confusion matrix
  cm <- confusionMatrix(predict(model, df_test),
                        df_test[[target]])
  cm
}
################################################################################
get_ROC_performance <- function(model, df_test, target) {
  # get probabilities
  predicted_probs <- predict(model, newdata = df_test, type = "prob")[, "No Failure"]
  pred <- prediction(predicted_probs, df_test[[target]])
  # get tpr and fpr to plot ROC
  perf <- performance(pred, "tpr", "fpr")
  perf
}
################################################################################
get_tpr_at_x_fpr <- function(model, df_test, target, x=0.1) {

  perf <- get_ROC_performance(model=model, df_test=df_test, target=target)
  
  # Extract TPR and FPR values
  fpr <- perf@x.values[[1]]
  tpr <- perf@y.values[[1]]
  
  # Interpolate to find TPR at the specified FPR value
  approx_tpr <- approx(fpr, tpr, xout = x)$y
  approx_tpr
}
################################################################################
plot_ROC_curves <- function(trained_models, model_names, df_test, target) {
  
  # Create an empty plot
  plot(0, 0, type = "n", xlim = c(0, 1), ylim = c(0, 1), 
       xlab = "False Positive Rate", ylab = "True Positive Rate", 
       main = "ROC curves comparison")
  
  grid(col = "grey", lty = "dotted")
  # Plot ROC curves for each model
  for (i in seq_along(trained_models)) {
    perf <- get_ROC_performance(trained_models[[i]], df_test, target)
    auc <- round(as.numeric(perf@y.values[[1]]), 3)  # Extract AUC value
    
    # Extract TPR and FPR values
    fpr <- perf@x.values[[1]]
    tpr <- perf@y.values[[1]]
    
    # Plot ROC curve
    lines(fpr, tpr, col = i, lwd = 2)
  }
  
  # Add legend
  legend("bottomright", legend = model_names, 
         col = seq_along(trained_models), lwd = 2, cex = 0.8)
}

################################################################################
plot_ROC_curves_cross_comparison <- function(trained_models_1, 
                                             trained_models_2,
                                             set_1_description,
                                             set_2_description,
                                             method_names,
                                             model_names, 
                                             df_test, target,
                                             df_test_2 = data.frame()) {

  
  # create a dictionary using setNames()
  method_model_dictionary <- setNames(model_names, method_names)
  
  # Create an empty plot
  plot(0, 0, type = "n", xlim = c(0, 1), ylim = c(0, 1), 
       xlab = "False Positive Rate", ylab = "True Positive Rate", 
       main = "ROC curves comparison")
  
  grid(col = "grey", lty = "dotted")
  legend_names <- character()
  
  # use same test set if another one is not given
  if (nrow(df_test_2)==0){df_test_2 <- df_test}
  
  color_index <- 1
  # plot ROC curves for the methods which are in "method_names" argument
  for (i in seq(1, length(trained_models_1))) {
    
    method_name_1 <- trained_models_1[[i]][["method"]]
    
    for (ii in seq(1, length(trained_models_2))) {
      
      method_name_2 <- trained_models_2[[ii]][["method"]]
      
      if (method_name_1==method_name_2 & method_name_2 %in% method_names){
        
        perf_1 <- get_ROC_performance(trained_models_1[[i]], 
                                      df_test, 
                                      target)
        
        perf_2 <- get_ROC_performance(trained_models_2[[ii]], 
                                      df_test_2, 
                                      target)
        
        # extract TPR and FPR values
        fpr_1 <- perf_1@x.values[[1]]
        tpr_1 <- perf_1@y.values[[1]]
        fpr_2 <- perf_2@x.values[[1]]
        tpr_2 <- perf_2@y.values[[1]]
        
        # plot ROC curve
        lines(fpr_1, tpr_1, col = color_index, lwd = 2, lty=1)
        lines(fpr_2, tpr_2, col = color_index, lwd = 2, lty=2)
        
        # make the right legends
        legend_names <- c(legend_names, 
                          paste(set_1_description, 
                            method_model_dictionary[[method_name_1]]))
        
        legend_names <- c(legend_names, 
                          paste(set_2_description, 
                            method_model_dictionary[[method_name_1]]))
        
        color_index <- color_index + 1
        }
      }
    }

  # Add legend
  legend("bottomright", legend = legend_names, 
         col = rep(seq_along(model_names), each = 2), 
         lwd = 2, lty = c(1, 2), cex = 0.8)
}

################################################################################
# Function to perform SMOTE on failure type, over sample based on type frequency
smote_for_failure_type <- function(train_data, target_col, positive_class, 
                                   subclass_type_col, features_cols,
                                   apprx_subclass_pop) {
  
  # Extract features and target variables
  train_features <- train_data[, features_cols]
  train_failure_type <- train_data[[subclass_type_col]]
  
  # Calculate the frequency of each Failure type
  failure_type_freq <- table(train_failure_type)
  
  # most common item in the failure type column (No Failure)
  majority_class <- names(which.max(failure_type_freq))
  
  # Find minority Failure types, exclude the most common one (No Failure)
  failure_types <- names(failure_type_freq[failure_type_freq != majority_class])
  
  # Perform SMOTE for each minority Failure type
  df_smote <- list()
  i <- 1
  for (failure_type in failure_types) {
    # Subset the data for the current Failure type
    subset_data <- train_data[train_failure_type == failure_type |
                                train_failure_type == majority_class, ]
    
    # find the correct dupsize based on the desired subclass population
    dupsize <- apprx_subclass_pop/failure_type_freq[[failure_type]]
    
    # Perform SMOTE on the subset
    smote_subset <- SMOTE(subset_data[, features_cols], 
                          target = subset_data[[target_col]],
                          dup_size = dupsize)
    
    # take the dataset with over-sampled minority and add a failure type column
    df_smote_subclss <- smote_subset$data
    df_smote_subclss[[subclass_type_col]] <- ifelse(
      df_smote_subclss$class == positive_class, 
      failure_type, majority_class)
    
    # Rename the class column to match the original target column name
    df_smote_subclss <- df_smote_subclss %>% 
      rename(!!target_col := class)
    
    # Combine SMOTE results for each Failure type
    df_smote[[i]] <- df_smote_subclss
    i <- i+1
  }
  
  # Combine SMOTE results for all minority Failure types
  df_smote <- do.call(rbind, df_smote)
  
  # remove duplicated rows due to majority class
  df_smote <- distinct(df_smote)
  
  # make both target columns factors again
  df_smote[[target_col]] <- as.factor(df_smote[[target_col]])
  df_smote[[subclass_type_col]] <- as.factor(df_smote[[subclass_type_col]])
  
  # return the final dataframe
  df_smote
}

################################################################################
# Function to perform DBSMOTE on failure type, over sample based on type frequency
DBSMOTE_for_failure_type <- function(train_data, target_col, positive_class, 
                                     subclass_type_col, features_cols,
                                     apprx_subclass_pop) {
  
  # Extract features and target variables
  train_features <- train_data[, features_cols]
  train_failure_type <- train_data[[subclass_type_col]]
  
  # Calculate the frequency of each Failure type
  failure_type_freq <- table(train_failure_type)
  
  # most common item in the failure type column (No Failure)
  majority_class <- names(which.max(failure_type_freq))
  
  # Find minority Failure types, exclude the most common one (No Failure)
  failure_types <- names(failure_type_freq[failure_type_freq != majority_class])
  
  # Perform SMOTE for each minority Failure type
  df_smote <- list()
  i <- 1
  for (failure_type in failure_types) {
    # Subset the data for the current Failure type
    subset_data <- train_data[train_failure_type == failure_type |
                                train_failure_type == majority_class, ]
    
    # find the correct dupsize based on the desired subclass population
    dupsize <- apprx_subclass_pop/failure_type_freq[[failure_type]]
    
    # Perform SMOTE on the subset
    smote_subset <- DBSMOTE(subset_data[, features_cols], 
                            target = subset_data[[target_col]],
                            dupSize = dupsize)
                            #K = 5,
                            #method="type2")
    
    # take the dataset with over-sampled minority and add a failure type column
    df_smote_subclss <- smote_subset$data
    df_smote_subclss[[subclass_type_col]] <- ifelse(
      df_smote_subclss$class == positive_class, 
      failure_type, majority_class)
    
    # Rename the class column to match the original target column name
    df_smote_subclss <- df_smote_subclss %>% 
      rename(!!target_col := class)
    
    # Combine SMOTE results for each Failure type
    df_smote[[i]] <- df_smote_subclss
    i <- i+1
  }
  
  # Combine SMOTE results for all minority Failure types
  df_smote <- do.call(rbind, df_smote)
  
  # remove duplicated rows due to majority class
  df_smote <- distinct(df_smote)
  
  # make both target columns factors again
  df_smote[[target_col]] <- as.factor(df_smote[[target_col]])
  df_smote[[subclass_type_col]] <- as.factor(df_smote[[subclass_type_col]])
  
  # return the final dataframe
  df_smote
}
################################################################################
# Function to perform ENN on failure type, in order to reduce noise in the data
apply_ENN <- function(df, target, features, 
                      k = 5, dist = "Euclidean") {
  
  df <- df[c(features, target)]
  
  # Prepare the formula
  formula <- as.formula(paste(target, "~", 
                              paste(features, collapse = " + ")))
  
  # Apply ENN
  res_enn <- ENNClassif(formula, df, k = k, dist = dist, Cl = "all")
  
  # Remove instances identified by ENN
  data_filtered <- df[-res_enn[[2]], ]
  
  data_filtered
}
################################################################################
# converting multiclass classification to binary classification
conver_to_binary_confusion_matrix <- function(multiclass_confusion_matrix){
  
  # Extract true negatives (TN)
  TN <- multiclass_confusion_matrix["No Failure", "No Failure"]
  
  # Calculate false positives (FP)
  FP <- sum(multiclass_confusion_matrix[, "No Failure"][-2])
  
  # Calculate false negatives (FN)
  FN <-  sum(multiclass_confusion_matrix["No Failure", -2 ])
  
  # Calculate true positives (TP)
  TP <- sum(diag(multiclass_confusion_matrix)) - TN
  
  # Construct the binary confusion matrix
  binary_confusion_matrix <- matrix(c(TP, FP, FN, TN), nrow = 2, byrow = TRUE,
                                    dimnames = list(c("Predicted Failure", 
                                                      "Predicted No Failure"),
                                                    c("Failure", "No Failure")))
  binary_confusion_matrix
}
################################################################################
# calculates f1 score after converting multiclass classification to binary
get_f1_score<- function(binary_confusion_matrix){
  
  precision <- binary_confusion_matrix[1,1]/(binary_confusion_matrix[1,1] + binary_confusion_matrix[1,2])
  recall <- binary_confusion_matrix[1,1]/(binary_confusion_matrix[1,1] + binary_confusion_matrix[2,1])
  F1 <- 2 * ( (precision*recall) / (precision+recall) )
  F1
}
################################################################################
# function hard voting ensemble 
predict_ensemble <- function(models, ensemble_methods, new_data) {
  
  predictions <- sapply(ensemble_methods, function(method) {
    predict(models[[which(methods==method)]], new_data)
  })
  
  # Combine predictions using majority voting
  majority_vote <- apply(predictions, 1, function(row) {
    names(which.max(table(row)))
  })
  majority_vote
}
```


# I. Introduction
This document is the generated report for the "Chose Your Own Project" capstone of the "HarvardX PH125.9x: Data Science: Capstone" course provided by HarvardX. In the intoducion section of this document, we will go over the motivations and goals behind the project, the chosen data set, problem definition, and the adapted strategy to approach the problem. In later sections, we will go over the exploratory data analysis (EDA), different modelling strategies, model performance comparisons and final results. The document will end with concluding remarks and suggestions of possible improvements for a potential future work.

## I.I Motivation
As the previous "Movielens" capstone project was essentially a regression problem, it was desired to have a classification problem for the final capstone project. Working on a **predictive maintenance** project presented a relevant and intellectually stimulating prospect for this capstone.
Predictive maintenance is a growing field of interest in data-driven engineering. The goal of predictive maintenance is to monitor the health state of an asset (e.g. a machine, a component thereof, or even a part of the infrastructure) through analysis of the data collected by sensors which monitor the asset of interest. For instance, if vibration levels of a component is under continuous monitoring by a sensor which sends the data to a database, and the machine learning models detect a change point or a trend in these vibration levels, the asset owner can send inspection personnel to investigate the alert, and possibly carry a preemptive maintenance action to avoid total failure of the asset. In this way, the resources of the inspection personnel is used more efficiently, because the periodic and/or reactive maintenance is replaced by the more targeted approach of **predictive maintenance**. In this project, we will work on a data set to classify the state of machines and predict which failure mode they have based on the provided data.

## I.II The Data set
The Data set chosen for this project is the **Machine Predictive Maintenance Classification** data set available on Kaggle website [1]. We can import the data set and have a first look after renaming the columns to convenient names:
```{r import data and show head, echo=TRUE}
# load data set
df_data <- read.csv("./data/predictive_maintenance.csv")

# rename columns to more convenient names
df_data <- df_data %>% 
  rename("product_id" = Product.ID,
         "type" = Type,
         "air_t" = Air.temperature..K.,
         "process_t" = Process.temperature..K.,
         "rpm" = Rotational.speed..rpm.,
         "torque" = Torque..Nm.,
         "wear" = Tool.wear..min.,
         "failure_numeric" = Target,
         "failure_type" = Failure.Type)

# show the head
knitr::kable(head(df_data), row.names = FALSE)
```
A quick check shows that the first two columns have unique values for every row. Therefore, they are not relevant for any modelling considerations:
```{r UDI and product ID cols, echo=TRUE}
n_distinct(df_data$UDI)
n_distinct(df_data$product_id)
```

According to the data set description [1], the **type** column describes the type of the machine and contains possible 3 values of L, M, and H, which stand for low, medium, and high quality, respectively. the **air_t** and **process_t** columns contain respectively the air temperature, and the machine processing temperatures in Kelvin. The **rpm** column stores the angular speed of the machine in revolutions per minute (rpm), while the **torque** columns contains the angular force in Nm. The **wear** column indicates the tool wear in minutes. The last 2 columns are the labels. The column **failure_numeric** contains binary values, 1 for machine failure and 0 for no failure, followed by the **failure_type** columns which indicates the mode of failure. Here, we can see the failure modes:
```{r failure modes, echo=TRUE}
unique(df_data$failure_type)
```
We can also check if the data set is balanced or not by defining and looking at the distribution in the **failure** column:
```{r unbalanced dataset, echo=TRUE}
# crate the failure label column form 0 and 1 ==> no failure and failure
df_data <- df_data %>%
  mutate(failure = ifelse(failure_numeric==1, "Failure", "No Failure"))

table(df_data$failure)
```
The data set contains 9661 observations without failure, and only 339 failure observations. This shows that the data set is quite unbalanced and a classification algorithm may be affected by this.

## I.III The Objective
The objective in this project is to use different data analysis, data transformation, feature engineering, and machine learning methods to classify and predict the failure of the machines. Although there are 6 distinct categories in the **failure_type** column, one of them is essentially the negative case ("No Failure"), while the other categories are the positive cases. In other words, this classification problem is inherently a binary problem from this point of view. Additionally, it is possible to imagine a real-world scenario where the machine owner is more interested in the health status of the machine rather than the specific failure mode.
It is important to mention that in our modelling efforts, we may use the multi-class classification approaches, but if the model predicts e.g. **Heat Dissipation Failure** while the real case is **Overstrain Failure**, we will still count it as a true positive. 
Since the data set is very unbalance, we will not use **accuracy** as the metric to evaluate the model performance. Instead, we will use **F1-score** for that purpose. As a reminder, the F1-score is calculated through the following formula:

\[
F1 = 2 \times\frac{recall \times precision}{recall+precision}
\]

Precision and recall are defined by the following formulas:

\[
Precision = \frac{TP}{TP+FP}
\]

\[
Recall = \frac{TP}{TP+FN}
\]

Additionally, we will use comparisons of receiver operating characteristic (ROC) curves to visualize the model performance. One more metric that will be calculated throughout some of the sections in this report is the true positive rate (TPR) at 10% false positive rate (FPR). This metric can helpful, because in a real-world scenario, the machine owner may require the data scientist to produce a model that does not have more than 10% false alarm rate, while maintaining a high true alarm rate. Please note that TPR is simply another name for recall, and FPR is defined as:
\[
FPR = \frac{FP}{TN+FP}
\]

As mentioned before, the objective in this project make the binary failure/no failure classification. We will try to achieve an F1-score of **0.95**. We will also use the ROC curves and the TPR at 10% FPR metric as helpful means that guide our modelling efforts.

\newpage


# II. Analysis
Prior to any modelling step, it is crucial to perform some level of EDA and visualization in order to be aware of the relationships and correlations that exist in our data. This will be the starting point in our journey to create a classification model. As the first step, let us create a detailed figure including multiple plots, so that we will see the big picture more clearly:

```{r pair plot, echo=TRUE, fig.height=7, fig.width=10}
# select columns for pair plot
df_pair <- df_data %>% 
  dplyr::select(-any_of(c("UDI", "failure", "failure_numeric",
                   "product_id", "failure_type")))

# create and show the pair plot
pair_plot <- ggpairs(df_pair,
                     mapping=aes(color=df_data$failure),
                     lower = list(continuous = wrap("points", alpha = 0.2), 
                                  combo = "box_no_facet"),
                     diag = list(continuous = wrap("densityDiag", alpha = 0.2))) +
  labs(title ="Feature pair-wise relations grid plot") + 
  theme(axis.text.x = element_text(angle = 90,))

pair_plot
```
```{r removing df_pair, include=FALSE}
# remove df_pair from environment as it's no longer needed
rm(df_pair)
```
This is a very dense figure, but one can already draw very useful insights from it. The first thing to notice is that there are many more blue points compared to red points in the subplots. This means the data set is extremely unbalanced and has a lot more "No Failure" cases than "Failure" ones. From the bar plot at the top left, one can see that L type machines have the most failures, followed by M and H type machines. The box plots on the left and top of the figure show us some rpm, torque, and wear have substantially different distributions for failed and not failed machines, regardless of the machine type. This is also visible in the density plots. The air and process temperatures also seem to be higher in case of failed machines. The figure also shows the correlation coefficients of each feature pair in both failure and no failure cases. In some pairs such as **air_t** vs **wear**, the difference in correlations for the 2 classes seem to be very high. However, one needs to take into consideration that the correlation values for failure cases are based on much smaller number of samples, and different failure types might actually have different correlation values. It is also clear from the figure that the relationship between torque and rpm is inverse. This is expected, because in machines higher torque always comes with a lower rotational speed assuming a constant power output.

Next, let us have a look at the distribution of the failure modes:
```{r failure mode bar plot, echo=TRUE, fig.height=6, fig.width=10}
# see number of different types of failure
df_data %>% 
  filter(failure_numeric==1) %>%
  group_by(failure_type) %>% 
  summarize(n_failure = n()) %>% 
  mutate(failure_type=reorder(failure_type,n_failure)) %>%
  ggplot(aes(x=failure_type, y=n_failure)) + 
  geom_col(color = "steelblue", fill="blue3") + 
  labs(x = "Failure type",
       y = "Number of failures",
       title ="Failure type vs Number of failures") + 
  coord_flip()
```

There are 2 issues one can observe in this plot: \newline
1) "No Failure" appears in the plot despite we filtered them out.\newline
2) "Random Failures" are not in the plot despite the exist in the data set as we discussed in the introduction section.

Therefore, we need to perform some data cleaning. We change the **failure_numeric** column to 0 and **failure** column to "No Failure" for all instances with "No Failure" in **failure_type** columns. We also change the failure to to "No Failure" for all instances with 0 in the **failure_numeric** column:

```{r data cleaning 1, echo=TRUE}

# reassign correct failure and failure_numeric to the problematic rows
df_data <- df_data %>% 
  mutate(failure_numeric = ifelse(failure_type=="No Failure", 0, 1),
         failure = ifelse(failure_type=="No Failure", "No Failure", failure))

# reassign correct failure_type to the random failure rows
df_data <- df_data %>% 
  mutate(failure_type = ifelse(failure=="No Failure", 
                               "No Failure", 
                               failure_type))
```
```{r convert to factor, include=FALSE}
# convert categorical features into factors instead of strings
df_data$type <- as.factor(df_data$type)
df_data$failure <- as.factor(df_data$failure)
df_data$failure_type <- as.factor(df_data$failure_type)
```
Next, let us visualize the correlations in a better way. This time we also want to include the machine type. However, the machine type in out data set is categorical. To overcome this, we do **one-hot encoding** of the type data, i.e. we encode the type data in a numeric way. After this encoding, we will have 3 new columns in our data set. Here, we see how these columns are related to the **type** column:
```{r one hot encoding, echo=TRUE}
# one-hot encoding the categorical type column using the self-defined function
df_data <- one_hot_encode(df_data = df_data, categorical_col = "type")

# show encoding
tail(df_data %>% dplyr::select(type, type_L, type_M, type_H))
```
Now that we have numerically encoded the machine types, we can make create a comprehensive correlation plot. Please note that the correlation coefficients are calculated using the Spearman method.
```{r column rearrangement, include=FALSE}
# re-arrange columns to have the target columns at the end 
all_cols <- names(df_data)
target_cols <- c("failure", "failure_type", "failure_numeric")
other_cols <- setdiff(all_cols, target_cols)
df_data <- df_data[, c(other_cols, target_cols)]
```
```{r correlation plot, echo=TRUE, fig.height=6, fig.width=10}
# select columns for correlations plot
df_corr <- df_data %>% 
  dplyr::select(-any_of(c("UDI", "type", "failure",
                   "product_id", "failure_type")))

# plot the correlations using spearman method
ggcorr(df_corr, label = TRUE, label_round=3, 
       method=c("pairwise", "spearman"),
       low = "brown2", 
       mid = "lightgrey", 
       high = "steelblue")
```
```{r delete the corr plot, include=FALSE}
# remove df_corr from environment as it's no longer needed
rm(df_corr)
```
In this plot, we can see that higher torque and wear are correlated with failure. Also, rpm is inversely correlated. There is some variation in correlation coefficients between failure and different machine types, however, the coefficients are too small to conclude anything.




